{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b059e31c-f649-44de-ab9c-8acee9766126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "textile_df = spark.read.csv(\"dbfs:/FileStore/shared_uploads/saran@uni-koblenz.de/textile.csv\", header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "#display(textile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93239e8c-c042-4fa9-9a7c-ea8b718f1bee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import expr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6a1466-26cf-4d5f-bf7a-8bcca5964d21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DF1  = textile_df.drop(\"Alpaca\", \"Other_animal\",\"Camel\",\"Other_regenerated\",\"Other_plant\",\"Jute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a470e1-53ed-449c-8b03-6b5885ffa60a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DF_1 = DF1\n",
    "\n",
    "DF_1 = DF_1.withColumn(\"Manufacturing_location_Africa\", when(DF_1[\"Manufacturing_location\"] == \"Africa\", 1).otherwise(0))\n",
    "DF_1 = DF_1.withColumn(\"Manufacturing_location_America\", when(DF_1[\"Manufacturing_location\"] == \"America\", 1).otherwise(0))\n",
    "DF_1 = DF_1.withColumn(\"Manufacturing_location_Asia\", when(DF_1[\"Manufacturing_location\"] == \"Asia\", 1).otherwise(0))\n",
    "DF_1 = DF_1.withColumn(\"Manufacturing_location_Europe\", when(DF_1[\"Manufacturing_location\"] == \"Europe\", 1).otherwise(0))\n",
    "DF_1 = DF_1.withColumn(\"Manufacturing_location_Unknown\", when((DF_1[\"Manufacturing_location\"].isNull()) | (DF_1[\"Manufacturing_location\"] == \"NaN\"), 1).otherwise(0))\n",
    "\n",
    "DF_1 = DF_1.withColumn(\"Drying_instruction_Linedry\", when(DF_1[\"Drying_instruction\"] == \"Line dry\", 1).otherwise(0))\n",
    "DF_1 = DF_1.withColumn(\"Drying_instruction_Dryclean\", when(DF_1[\"Drying_instruction\"] == \"Dry clean\", 1).otherwise(0))\n",
    "DF_1 = DF_1.withColumn(\"Drying_instruction_Tumble\", when((DF_1[\"Drying_instruction\"] == \"Tumble dry_ low\") | (DF_1[\"Drying_instruction\"] == \"Tumble dry_ low\"),1).otherwise(0))\n",
    "\n",
    "\n",
    "DF_1 = DF_1.withColumn(\"Washing_instruction_Machinehot\", when((DF_1[\"Washing_instruction\"] == \"Machine wash_ warm\") | (DF_1[\"Washing_instruction\"] == \"Machine wash_ hot\"),1).otherwise(0))\n",
    "DF_1 = DF_1.withColumn(\"Washing_instruction_Machinecold\", when(DF_1[\"Washing_instruction\"] == \"Machine wash_ cold\",1).otherwise(0))\n",
    "DF_1 = DF_1.withColumn(\"Washing_instruction_Handwash\", when(DF_1[\"Washing_instruction\"] == \"Hand wash\",1).otherwise(0))\n",
    "DF_1 = DF_1.withColumn(\"Washing_instruction_Dryclean\", when(DF_1[\"Washing_instruction\"] == \"Dry clean\",1).otherwise(0))\n",
    "\n",
    "DF_1 = DF_1.drop(\"Drying_instruction\")\n",
    "DF_1 = DF_1.drop(\"Manufacturing_location\")\n",
    "DF_1 = DF_1.drop(\"Washing_instruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859d0f92-7fb5-482e-b9f4-ca5281608fa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 82.38%\nPrecision = 82.89%\nRecall = 82.38%\nF1 Score = 82.18%\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = DF_1.randomSplit([.8, .2], seed=42)\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") | (dataType == \"integer\") & (field != \"EI\"))]\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "#Hyper-Parameter Tuning\n",
    "num_trees = 50\n",
    "max_depth = 15\n",
    "max_bins = 30\n",
    "feature_subset_strategy = \"all\"\n",
    "subsampling_rate = 0.8\n",
    "min_instances_per_node = 2\n",
    "min_info_gain = 0.01\n",
    "\n",
    "rf_classifier = RandomForestClassifier(labelCol=\"EI\", \n",
    "                                       numTrees=num_trees, \n",
    "                                       maxDepth=max_depth, \n",
    "                                       maxBins=max_bins, \n",
    "                                       featureSubsetStrategy=feature_subset_strategy, \n",
    "                                       subsamplingRate=subsampling_rate, \n",
    "                                       minInstancesPerNode=min_instances_per_node, \n",
    "                                       minInfoGain=min_info_gain)\n",
    "\n",
    "# Update the stages for the pipeline\n",
    "stages = [string_indexer, vec_assembler, rf_classifier]\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "pred_df = pipeline_model.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"EI\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = evaluator.evaluate(pred_df, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "# Calculate precision\n",
    "precision = evaluator.evaluate(pred_df, {evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Calculate recall\n",
    "recall = evaluator.evaluate(pred_df, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = evaluator.evaluate(pred_df, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy on test data = {:.2%}\".format(accuracy))\n",
    "print(\"Precision = {:.2%}\".format(precision))\n",
    "print(\"Recall = {:.2%}\".format(recall))\n",
    "print(\"F1 Score = {:.2%}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dcd022e-8c4d-42b0-80cb-86b09c2151e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a38ee58-035d-4a01-bf5b-0d29d0f6cb8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "    'numTrees': hp.quniform('numTrees', 10, 100, 1),\n",
    "    'maxDepth': hp.quniform('maxDepth', 5, 20, 1),\n",
    "    'maxBins': hp.quniform('maxBins', 23, 50, 1),\n",
    "    'featureSubsetStrategy': hp.choice('featureSubsetStrategy', ['auto', 'all', 'sqrt', 'log2']),\n",
    "    'subsamplingRate': hp.uniform('subsamplingRate', 0.5, 1.0),\n",
    "    'minInstancesPerNode': hp.quniform('minInstancesPerNode', 1, 10, 1),\n",
    "    'minInfoGain': hp.uniform('minInfoGain', 0.0, 0.1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d62fbb9a-b012-4b8e-b263-7875387b2a39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]\r  2%|▏         | 1/50 [01:31<1:14:27, 91.18s/trial, best loss: -0.49740932642487046]\r  4%|▍         | 2/50 [03:14<1:18:48, 98.50s/trial, best loss: -0.772020725388601]  \r  6%|▌         | 3/50 [04:55<1:17:47, 99.32s/trial, best loss: -0.772020725388601]\r  8%|▊         | 4/50 [06:21<1:12:22, 94.41s/trial, best loss: -0.772020725388601]\r 10%|█         | 5/50 [07:48<1:08:34, 91.43s/trial, best loss: -0.772020725388601]\r 12%|█▏        | 6/50 [09:24<1:08:19, 93.16s/trial, best loss: -0.772020725388601]\r 14%|█▍        | 7/50 [10:54<1:05:56, 92.01s/trial, best loss: -0.772020725388601]\r 16%|█▌        | 8/50 [12:24<1:03:56, 91.34s/trial, best loss: -0.772020725388601]\r 18%|█▊        | 9/50 [14:00<1:03:22, 92.74s/trial, best loss: -0.772020725388601]\r 20%|██        | 10/50 [15:36<1:02:39, 93.98s/trial, best loss: -0.772020725388601]\r 22%|██▏       | 11/50 [17:21<1:03:17, 97.37s/trial, best loss: -0.772020725388601]\r 24%|██▍       | 12/50 [19:04<1:02:37, 98.89s/trial, best loss: -0.772020725388601]\r 26%|██▌       | 13/50 [20:37<59:52, 97.09s/trial, best loss: -0.772020725388601]  \r 28%|██▊       | 14/50 [22:11<57:45, 96.26s/trial, best loss: -0.772020725388601]\r 30%|███       | 15/50 [23:44<55:32, 95.20s/trial, best loss: -0.772020725388601]\r 32%|███▏      | 16/50 [25:22<54:24, 96.01s/trial, best loss: -0.772020725388601]\r 34%|███▍      | 17/50 [27:17<56:02, 101.89s/trial, best loss: -0.772020725388601]\r 36%|███▌      | 18/50 [28:55<53:40, 100.64s/trial, best loss: -0.772020725388601]\r 38%|███▊      | 19/50 [30:35<51:51, 100.38s/trial, best loss: -0.772020725388601]\r 40%|████      | 20/50 [32:13<49:52, 99.76s/trial, best loss: -0.772020725388601] \r 42%|████▏     | 21/50 [34:14<51:20, 106.24s/trial, best loss: -0.772020725388601]\r 44%|████▍     | 22/50 [36:32<53:55, 115.56s/trial, best loss: -0.772020725388601]\r 46%|████▌     | 23/50 [38:29<52:15, 116.12s/trial, best loss: -0.772020725388601]\r 48%|████▊     | 24/50 [40:25<50:15, 115.97s/trial, best loss: -0.772020725388601]\r 50%|█████     | 25/50 [42:13<47:20, 113.60s/trial, best loss: -0.772020725388601]\r 52%|█████▏    | 26/50 [44:06<45:22, 113.44s/trial, best loss: -0.772020725388601]\r 54%|█████▍    | 27/50 [46:01<43:42, 114.02s/trial, best loss: -0.772020725388601]\r 56%|█████▌    | 28/50 [48:12<43:37, 119.00s/trial, best loss: -0.772020725388601]\r 58%|█████▊    | 29/50 [50:13<41:49, 119.52s/trial, best loss: -0.772020725388601]\r 60%|██████    | 30/50 [52:44<43:04, 129.24s/trial, best loss: -0.7927461139896373]\r 62%|██████▏   | 31/50 [54:54<40:58, 129.39s/trial, best loss: -0.7927461139896373]\r 64%|██████▍   | 32/50 [56:56<38:10, 127.25s/trial, best loss: -0.7927461139896373]\r 66%|██████▌   | 33/50 [59:47<39:42, 140.14s/trial, best loss: -0.7927461139896373]\r 68%|██████▊   | 34/50 [1:02:15<38:03, 142.71s/trial, best loss: -0.7927461139896373]\r 70%|███████   | 35/50 [1:03:16<29:31, 118.08s/trial, best loss: -0.7927461139896373]\r 72%|███████▏  | 36/50 [1:06:02<30:52, 132.36s/trial, best loss: -0.7927461139896373]\r 74%|███████▍  | 37/50 [1:08:42<30:29, 140.73s/trial, best loss: -0.7927461139896373]\r 76%|███████▌  | 38/50 [1:11:05<28:18, 141.50s/trial, best loss: -0.7927461139896373]\r 78%|███████▊  | 39/50 [1:12:34<23:03, 125.75s/trial, best loss: -0.7927461139896373]\r 80%|████████  | 40/50 [1:14:03<19:06, 114.62s/trial, best loss: -0.7927461139896373]\r 82%|████████▏ | 41/50 [1:15:27<15:48, 105.43s/trial, best loss: -0.7927461139896373]\r 84%|████████▍ | 42/50 [1:16:54<13:18, 99.81s/trial, best loss: -0.7927461139896373] \r 86%|████████▌ | 43/50 [1:18:31<11:33, 99.03s/trial, best loss: -0.7927461139896373]\r 88%|████████▊ | 44/50 [1:20:04<09:43, 97.31s/trial, best loss: -0.7927461139896373]\r 90%|█████████ | 45/50 [1:21:28<07:47, 93.42s/trial, best loss: -0.7927461139896373]\r 92%|█████████▏| 46/50 [1:22:58<06:09, 92.26s/trial, best loss: -0.7927461139896373]\r 94%|█████████▍| 47/50 [1:24:24<04:31, 90.54s/trial, best loss: -0.7927461139896373]\r 96%|█████████▌| 48/50 [1:25:50<02:58, 89.08s/trial, best loss: -0.7927461139896373]\r 98%|█████████▊| 49/50 [1:27:20<01:29, 89.34s/trial, best loss: -0.7927461139896373]\r100%|██████████| 50/50 [1:28:43<00:00, 87.50s/trial, best loss: -0.7927461139896373]\r100%|██████████| 50/50 [1:28:43<00:00, 106.48s/trial, best loss: -0.7927461139896373]\nBest parameters: {'featureSubsetStrategy': 1, 'maxBins': 42.0, 'maxDepth': 18.0, 'minInfoGain': 0.022677530041118416, 'minInstancesPerNode': 3.0, 'numTrees': 89.0, 'subsamplingRate': 0.9975054905192632}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    # Convert hyperopt params to int where necessary\n",
    "    params['numTrees'] = int(params['numTrees'])\n",
    "    params['maxDepth'] = int(params['maxDepth'])\n",
    "    params['maxBins'] = int(params['maxBins'])\n",
    "    params['minInstancesPerNode'] = int(params['minInstancesPerNode'])\n",
    "    \n",
    "    # Configure Random Forest Classifier with hyperparameters\n",
    "    rf_classifier = RandomForestClassifier(labelCol=\"EI\", **params)\n",
    "\n",
    "    # Update the stages for the pipeline\n",
    "    stages = [string_indexer, vec_assembler, rf_classifier]\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    pred_df = pipeline_model.transform(test_df)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"EI\", predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(pred_df, {evaluator.metricName: \"accuracy\"})\n",
    "    return -accuracy  # Minimize negative accuracy (maximize accuracy)\n",
    "\n",
    "# Use Trials for single-machine optimization\n",
    "trials = Trials()\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=tpe.suggest,\n",
    "                   max_evals=50,\n",
    "                   trials=trials)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36865195-7cb3-4e79-ba0e-f8784ae13254",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Evaluation Metrics:\nAccuracy on test data = 79.27%\nPrecision = 82.64%\nRecall = 79.27%\nF1 Score = 78.04%\n"
     ]
    }
   ],
   "source": [
    "# Convert best_params to int where necessary\n",
    "best_params['numTrees'] = int(best_params['numTrees'])\n",
    "best_params['maxDepth'] = int(best_params['maxDepth'])\n",
    "best_params['maxBins'] = int(best_params['maxBins'])\n",
    "best_params['minInstancesPerNode'] = int(best_params['minInstancesPerNode'])\n",
    "best_params['featureSubsetStrategy'] = \"all\"\n",
    "\n",
    "# Train the final model using the best parameters\n",
    "rf_classifier_best = RandomForestClassifier(labelCol=\"EI\", **best_params)\n",
    "stages_best = [string_indexer, vec_assembler, rf_classifier_best]\n",
    "pipeline_best = Pipeline(stages=stages_best)\n",
    "pipeline_model_best = pipeline_best.fit(train_df)\n",
    "pred_df_best = pipeline_model_best.transform(test_df)\n",
    "\n",
    "# Evaluate the final model\n",
    "accuracy_best = evaluator.evaluate(pred_df_best, {evaluator.metricName: \"accuracy\"})\n",
    "precision_best = evaluator.evaluate(pred_df_best, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall_best = evaluator.evaluate(pred_df_best, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1_score_best = evaluator.evaluate(pred_df_best, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Print evaluation metrics for the best model\n",
    "print(\"Best Model Evaluation Metrics:\")\n",
    "print(\"Accuracy on test data = {:.2%}\".format(accuracy_best))\n",
    "print(\"Precision = {:.2%}\".format(precision_best))\n",
    "print(\"Recall = {:.2%}\".format(recall_best))\n",
    "print(\"F1 Score = {:.2%}\".format(f1_score_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6860fe7-2aa4-44b6-bfce-3ee312d4b04f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HyperOpt RF_ParameterTuning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
